---
title: "Bé Thỏ (Phần 3) - Đêm 500 ngàn connections"
title_vi: "Bé Thỏ (Phần 3) - Đêm 500 ngàn connections"
title_en: "Little Rabbit (Part 3) - The Night of 500,000 Connections"
excerpt_vi: "8h09 PM, 11/01/2023. SOC gọi điện. 500,000 connections đổ vào RabbitMQ. 1.8 triệu transactions lost. Đây là câu chuyện về đêm dài nhất của team tôi."
excerpt_en: "8:09 PM, January 11, 2023. SOC called. 500,000 connections flooding RabbitMQ. 1.8 million transactions lost. This is the story of my team's longest night."
date: 2025-01-18
tags:
  - RabbitMQ
  - Incident Response
  - Architecture
  - Debugging
  - Series Bé Thỏ
thumbnail: https://images.unsplash.com/photo-1504639725590-34d0984388bd?w=800
---

# Bé Thỏ (Phần 3) - Đêm 500 ngàn connections

## 8 giờ 09 phút tối

Ngày 11 tháng 1 năm 2023.

Tôi đang chuẩn bị rời văn phòng thì điện thoại rung. SOC gọi. Giọng người trực nghe căng thẳng hơn bình thường: *"Anh ơi, RabbitMQ có vấn đề. Message rate về 0."*

Message rate về 0. Không phải giảm xuống. Là **0**.

Tôi bật laptop, truy cập dashboard. Và những gì hiện ra trên màn hình khiến tôi đông cứng tại chỗ.

Connections đang ở mức 500,000. Bình thường chỉ 50,000. Gấp 10 lần.

Message rate nhảy nhót như nhịp tim của người sắp chết - lên được vài trăm rồi rớt về 0, lên rồi lại rớt. Hệ thống đang thở hắt ra.

Tôi gọi ngay cho anh CTO. *"Anh ơi, rabbit chết rồi."*

![Crisis](https://images.unsplash.com/photo-1504639725590-34d0984388bd?w=800)

## Con số ám ảnh

Trong lúc chờ team online, tôi nhìn vào dashboard business metrics. Và đây là lúc tim tôi thực sự đập nhanh hơn.

**1.8 triệu transactions** đã bị lost. **446,000 users** bị ảnh hưởng.

Tất cả services sử dụng RabbitMQ đều chết. Core banking, payment, user service, notification. Mọi thứ. Toàn bộ hệ thống payment đang trong trạng thái "clinically dead" - còn thở nhưng không còn sống.

## Truy tìm thủ phạm

Khi cả team đã online, câu hỏi đầu tiên là: **Cái gì đã trigger incident này?**

Kiểm tra logs, chúng tôi tìm thấy manh mối đầu tiên. Khoảng 8 giờ tối - cùng thời điểm với incident - có một đợt lệnh lớn từ hệ thống Core kết nối vào Database, gây ra Lock Agent. Connection database tăng đột biến.

Nhưng đó chỉ là **trigger**, không phải **root cause**. Database có vấn đề thì RabbitMQ liên quan gì?

Câu trả lời nằm ở một hiện tượng mà sau này chúng tôi gọi là **Connection Storm**.

## Hiệu ứng bầy đàn chết người

Hãy tưởng tượng một cây cầu nhỏ bắc qua sông. Bình thường, 100 người đi qua cầu mỗi phút - thoải mái, không chen lấn.

Một ngày, có sự cố nhỏ khiến cầu tắc nghẽn trong 30 giây. Những người đang chờ bắt đầu sốt ruột. Họ cố chen lên để qua cầu nhanh hơn. Nhưng càng chen, cầu càng tắc. Càng tắc, càng nhiều người chen.

Và rồi, cầu sập.

**Đó chính xác là những gì đã xảy ra với RabbitMQ của chúng tôi.**

Khi Database lock xảy ra, một số services phụ thuộc vào database bắt đầu chậm lại. Chậm lại có nghĩa là giữ connection RabbitMQ lâu hơn. Connection bị giữ lâu hơn có nghĩa là RabbitMQ bắt đầu quá tải.

Và đây là lúc config "auto reconnect" của chúng tôi trở thành hung thủ.

```java
// Config lúc đó
.setReconnectAttempts(Integer.MAX_VALUE)  // Retry VÔ HẠN
.setReconnectInterval(1000L)               // Cứ 1 giây retry lại
```

`Integer.MAX_VALUE`. Retry vô hạn. Mỗi giây một lần.

Khi RabbitMQ bắt đầu chậm, các services bắt đầu timeout. Và khi timeout, chúng làm gì? **Reconnect.** Ngay lập tức. Không chờ đợi. Không suy nghĩ.

100 workers, mỗi worker có thể tạo đến 3,000 connections. Tất cả đồng loạt reconnect. Không phải 1 lần. Mà là vô số lần mỗi giây.

RabbitMQ vốn đã quá tải, giờ phải xử lý thêm hàng trăm ngàn connection requests mỗi giây. Nó càng chậm hơn. Càng nhiều timeout. Càng nhiều reconnect.

**Vicious cycle. Bầy đàn hoảng loạn.**

Trong vòng vài phút, hệ thống từ "hơi chậm" đã biến thành "chết hoàn toàn".

![Stampede](https://images.unsplash.com/photo-1582139329536-e7284fece509?w=800)

## TIME_WAIT - Kẻ giết người thầm lặng

Nhưng câu chuyện chưa dừng lại ở đó.

Khi kiểm tra trên HAProxy, chúng tôi phát hiện thêm một vấn đề: hàng trăm ngàn connections đang ở trạng thái TIME_WAIT.

Để hiểu TIME_WAIT, bạn cần biết cách TCP connection hoạt động. Khi một connection bị đóng, nó không biến mất ngay lập tức. Nó ở trạng thái "chờ" khoảng 60 giây để đảm bảo không có packet nào bị mất trong quá trình đóng.

Bình thường, điều này không phải vấn đề. Nhưng với 500,000 connections liên tục đóng và mở lại mỗi giây, số lượng TIME_WAIT tăng lên không kiểm soát. Mỗi TIME_WAIT connection vẫn chiếm một port, một file descriptor, một phần memory.

Hệ thống đang chết đuối trong biển connections của chính nó.

## Quyết định khó khăn nhất

Đến thời điểm này, chúng tôi đứng trước một quyết định khó khăn.

Thông thường, khi có incident, bạn cố gắng fix vấn đề mà không shutdown service. Downtime là kẻ thù. Mỗi phút downtime là tiền bạc, là uy tín, là khách hàng.

Nhưng nhìn vào tình hình hiện tại, anh CTO đưa ra quyết định mà không ai muốn nghe: *"Stop hết đi. Cả HAProxy lẫn RabbitMQ."*

Stop hoàn toàn. Không phải restart. Không phải graceful shutdown. **Stop.**

Đây là quyết định mà sau này tôi hiểu là hoàn toàn đúng đắn. Khi hệ thống đang trong trạng thái "thrashing" - khi mỗi hành động khắc phục chỉ làm tình hình tồi tệ hơn - cách duy nhất là **dừng lại hoàn toàn và bắt đầu lại từ đầu**.

Giống như khi máy tính bị treo cứng, đôi khi cách duy nhất là tắt nguồn và bật lại.

## Hồi sinh từ tro tàn

Sau khi stop cả HAProxy và RabbitMQ, chúng tôi bắt đầu quá trình "hồi sinh" có kiểm soát.

Bước đầu tiên: **giới hạn connections**. Trước đó, mỗi IP có thể tạo đến 3,000 connections. Chúng tôi giảm xuống 300. Một phần mười. Mục đích không phải để hạn chế vĩnh viễn, mà để đảm bảo rằng khi hệ thống start lại, không có ai có thể tạo ra connection storm lần nữa.

```bash
# Config mới cho HAProxy
stick-table type ip size 100k expire 30s store conn_cur
tcp-request connection reject if { src_conn_cur ge 300 }
```

Bước thứ hai: **start RabbitMQ trước**. Kiểm tra trạng thái. Đảm bảo nó stable. Đảm bảo số connections TIME_WAIT đã giảm về mức bình thường.

Bước thứ ba: **start HAProxy**. Nhưng vẫn chưa có traffic nào vào vì tất cả services vẫn đang bị scale down về 0.

Bước cuối cùng và quan trọng nhất: **start services theo thứ tự ưu tiên**. Không phải tất cả cùng lúc. Core banking trước. Rồi payment. Rồi user profile. Rồi authentication. Cuối cùng mới đến các services phụ.

Mỗi lần start một nhóm services, chúng tôi dừng lại, monitor. Connections có tăng quá nhanh không? Message rate có ổn định không? Chỉ khi mọi thứ OK, mới tiếp tục start nhóm tiếp theo.

Quá trình này mất gần 2 giờ. Nhưng khi kết thúc, hệ thống đã sống lại. Stable. Controllable.

![Recovery](https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800)

## Sai lầm chết người: Retry vô hạn

Sau incident, khi ngồi lại phân tích, một câu hỏi cứ ám ảnh tôi: **Tại sao chúng tôi lại config reconnect vô hạn?**

Câu trả lời đau lòng là: vì chúng tôi nghĩ đó là "resilient".

Logic lúc đó nghe có vẻ đúng: *"Nếu connection bị mất, cứ reconnect cho đến khi thành công. Service phải luôn available. Never give up."*

Nhưng "never give up" trong context của distributed systems lại là một ý tưởng tồi tệ.

Hãy nghĩ về điều này: nếu RabbitMQ đang overload, và 1,000 clients đồng loạt retry mỗi giây, bạn đang làm gì? Bạn đang **DDoS chính hệ thống của mình**.

Reconnect vô hạn không phải resilience. Đó là **tự sát tập thể**.

Config đúng phải là:

```java
.setReconnectAttempts(5)        // Chỉ retry 5 lần
.setReconnectInterval(10000L)   // Mỗi lần cách nhau 10 giây
```

Nếu 5 lần reconnect trong 50 giây mà vẫn không được, **dừng lại**. Log error. Alert. Để circuit breaker xử lý. Để human intervention vào cuộc.

Đừng cố reconnect vô hạn. Bạn không phải anh hùng. Bạn đang làm tình hình tồi tệ hơn.

## Bài học về kiến trúc

Incident này dạy chúng tôi nhiều điều, nhưng có một bài học vượt ra ngoài kỹ thuật: **không nên đặt tất cả trứng vào một giỏ**.

Trước incident, tất cả services - từ critical đến non-critical - đều dùng chung một RabbitMQ cluster. Payment và notification cùng một chỗ. Core banking và analytics cùng một chỗ.

Điều này có nghĩa là: khi RabbitMQ chết, **TẤT CẢ** đều chết. Không có cách nào để "hy sinh" notification để "cứu" payment.

Sau incident, chúng tôi tách thành hai clusters:
- **Cluster chính**: Chỉ cho services critical - payment, core banking, user authentication
- **Cluster phụ**: Cho services không critical - notification, logging, analytics

Nếu cluster phụ có vấn đề, users vẫn có thể thanh toán. Họ chỉ không nhận được notification ngay lập tức - một trade-off chấp nhận được.

Đây là nguyên tắc **graceful degradation**: khi hệ thống gặp vấn đề, nó không chết hoàn toàn, mà "giảm cấp" một cách có kiểm soát. Tính năng quan trọng nhất được bảo vệ. Tính năng ít quan trọng hơn có thể tạm thời bị hy sinh.

## Playbook - Thứ không ai muốn viết nhưng ai cũng cần

Một trong những việc đầu tiên sau incident là viết **playbook** - một tài liệu chi tiết về cách xử lý khi RabbitMQ gặp sự cố tương tự.

Playbook không phải để đọc cho vui. Nó để dùng lúc 2 giờ sáng, khi bạn vừa bị đánh thức bởi PagerDuty, mắt còn mờ, não chưa tỉnh. Lúc đó, bạn cần một checklist rõ ràng: bước 1 làm gì, bước 2 làm gì, verify như thế nào.

Chúng tôi cũng bắt đầu drill định kỳ - mỗi quý một lần, giả lập incident và chạy theo playbook. Không phải để "cho vui", mà để đảm bảo rằng khi incident thật xảy ra, mọi người đều biết mình phải làm gì.

Drill đầu tiên, mất 45 phút để recovery. Drill thứ hai, 30 phút. Drill gần nhất, chỉ 15 phút.

**Practice makes perfect.** Đặc biệt là với những thứ bạn không muốn phải làm.

![Playbook](https://images.unsplash.com/photo-1434030216411-0b793f4b4173?w=800)

## Nhìn lại

Đêm 11/01/2023 là đêm dài nhất của team tôi.

1.8 triệu transactions. 446,000 users. Những con số ám ảnh mà đến giờ tôi vẫn nhớ như in.

Nhưng từ đêm đó, chúng tôi học được những bài học mà không sách vở nào dạy:

**Bé Thỏ rất mạnh, nhưng cũng rất fragile** khi không được configure đúng cách. Một config "reconnect vô hạn" tưởng chừng vô hại có thể giết chết cả hệ thống.

**Connection storm là kẻ giết người thầm lặng.** Nó không đến từ bên ngoài, mà từ chính services của bạn. Chính sự "cố gắng" của services đã giết chết hệ thống.

**Đôi khi, cách tốt nhất để fix là stop hoàn toàn.** Không phải lúc nào cũng cần phải "fix on the fly". Khi hệ thống đang thrashing, dừng lại và bắt đầu lại từ đầu có thể là lựa chọn đúng đắn nhất.

**Graceful degradation là bắt buộc, không phải optional.** Tách biệt critical và non-critical services. Khi có sự cố, biết cái gì cần được bảo vệ và cái gì có thể hy sinh.

Bé Thỏ vẫn là người bạn đồng hành đáng tin cậy của chúng tôi. Nhưng giờ đây, chúng tôi hiểu bé hơn. Tôn trọng bé hơn. Và quan trọng nhất - **không bao giờ chủ quan với bé nữa**.

---

## Chưa hết

Nhưng câu chuyện về Bé Thỏ chưa kết thúc ở đây.

Vài tháng sau, một vấn đề mới xuất hiện. Vào những ngày cao điểm - ngày 5, ngày 10 hàng tháng - hệ thống lại nghẽn. Nhưng lần này, điều kỳ lạ là **mọi metrics đều bình thường**.

Server nói xử lý nhanh. Producer nói gọi chậm. DevOps nói RabbitMQ không có vấn đề.

Ba teams, ba bộ metrics, và ai cũng đúng. Nhưng hệ thống vẫn chậm.

Ai đúng? Ai sai? Và một công thức toán học đơn giản có tên **Little's Law** đã giúp CTO nhìn ra điều gì mà không ai thấy?

Câu chuyện này, tôi sẽ kể trong phần cuối cùng của series.

---

## Series "Bé Thỏ"

| Phần | Tiêu đề | Bài học chính |
|------|---------|---------------|
| [Phần 1](/vi/blog/be-tho-part-1) | Khi HTTP không còn đủ | Competing consumers, natural load balancing |
| [Phần 2](/vi/blog/be-tho-part-2) | Những cái bẫy chết người | Singleton pattern, channel/queue management |
| [Phần 3](/vi/blog/be-tho-part-3) | Đêm 500 ngàn connections | Connection storm, graceful degradation |
| [Phần 4](/vi/blog/be-tho-part-4) | Cuộc chiến không có kẻ thắng | Little's Law, accept uncertainty |

*Stay tuned cho phần cuối cùng!*
