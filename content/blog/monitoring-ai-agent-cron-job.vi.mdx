---
title: "3 Giờ Sáng, 47 Alert, và Bài Học Về AI Trong Monitoring"
title_vi: "3 Giờ Sáng, 47 Alert, và Bài Học Về AI Trong Monitoring"
title_en: "3 AM, 47 Alerts, and Lessons About AI in Monitoring"
excerpt_vi: "Đêm đó Tiến nhận 47 alert cùng lúc. Không ai biết bắt đầu từ đâu. Cho đến khi team học được: có những việc cron job phải làm, có những việc chỉ AI mới xử lý nổi - và đừng bao giờ nhầm lẫn hai thứ đó."
excerpt_en: "That night Tien received 47 alerts at once. No one knew where to start. Until the team learned: some things cron jobs must handle, some things only AI can process - and never confuse the two."
date: 2026-02-10
tags:
  - Monitoring
  - Incident Response
  - AI
  - DevOps
  - Architecture
thumbnail: https://images.unsplash.com/photo-1551288049-bebda4e38f71?w=800
---

3 giờ 7 phút sáng. Điện thoại Tiến rung.

Rồi rung nữa. Rồi rung liên tục không dừng.

Tiến mở mắt, với tay lấy điện thoại. Màn hình sáng lóa trong bóng tối: **47 notification từ PagerDuty.**

Database timeout. API latency spike. Queue backlog. 5xx error tăng vọt. Memory pressure trên 3 node. Certificate warning. Consumer lag. Connection pool exhausted.

Tiến ngồi dậy, cố gắng tỉnh táo. Đầu óc vẫn còn mơ màng. Nhìn vào đống alert hỗn loạn, một câu hỏi lởn vởn trong đầu: *"Bắt đầu từ đâu?"*

---

## Khi monitoring trở thành noise

Tiến là on-call engineer tuần đó. Bạn ấy có 5 năm kinh nghiệm, không phải lần đầu nhận alert đêm. Nhưng 47 alert cùng lúc thì... chưa bao giờ.

Tiến bắt đầu đọc từng alert một. Database timeout - có thể là root cause. API latency - có thể là hậu quả. Queue backlog - cũng có thể là hậu quả. Memory pressure - có liên quan không? Certificate warning - chắc không liên quan, nhưng sao lại xuất hiện cùng lúc?

30 phút trôi qua. Tiến vẫn đang mò mẫm.

Lúc đó Hiếu - senior engineer trong team - tỉnh dậy vì thấy Slack notification. Hiếu nhảy vào channel và nói:

*"Tiến ơi, nhìn vào timeline đi. DB connection pool exhausted lúc 3:01. Mọi thứ khác là domino effect. Focus vào DB trước."*

5 phút sau, Tiến tìm ra vấn đề: một deployment lúc 2:58 sáng đã thay đổi connection pool config. Connection limit giảm từ 100 xuống 10 do typo trong config file.

Rollback. Restart. Production recover.

Nhưng câu hỏi lớn hơn vẫn còn đó: **Tại sao Tiến mất 30 phút mò mẫm trong khi Hiếu chỉ mất 30 giây để nhìn ra?**

Và quan trọng hơn: **Tại sao hệ thống monitoring không tự làm được việc đó?**

---

## Bài học từ đêm hôm đó

Sáng hôm sau, cả team họp postmortem.

Hùng - DevOps lead - mở đầu: "Hệ thống monitoring của chúng ta đã làm đúng việc của nó. Nó detect được tất cả các vấn đề. Nhưng nó gửi 47 alert riêng lẻ thay vì nói cho Tiến biết: 'Này, 46 cái này là hậu quả, chỉ có 1 cái là root cause.'"

Tiến gật đầu. "Em mất 30 phút vì không biết cái nào quan trọng nhất. Alert nào cũng đỏ, cái nào cũng critical."

Hiếu thêm vào: "Mình biết ngay vì mình đã gặp pattern này trước. DB connection pool exhausted luôn là điểm khởi đầu của dây chuyền domino. Nhưng kinh nghiệm này nằm trong đầu mình, không nằm trong hệ thống."

Im lặng một lúc.

Rồi Hùng nói: "Vậy giải pháp là gì? Dùng AI để thông minh hóa alerting?"

Câu hỏi đó mở ra một cuộc thảo luận dài - và những bài học mà tôi muốn chia sẻ.

---

## Hai tầng của monitoring: Hệ thần kinh và Bộ não

Sau buổi postmortem đó, team nhận ra hệ thống monitoring có hai tầng rất khác nhau.

**Tầng 1 - Hệ thần kinh.** Giống như khi bạn chạm tay vào lửa, dây thần kinh phản xạ ngay lập tức - không cần suy nghĩ. CPU > 95%? Alert. Disk > 90%? Alert. Service không response? Alert. Nhanh. Đơn giản. Đáng tin cậy. Không có chỗ cho sự sáng tạo.

**Tầng 2 - Bộ não.** Giống như khi bạn nhận ra "à, tay đang đau vì vừa chạm lửa, và cái nồi đang sôi vì quên tắt bếp, và khói đang lên vì nước trào ra." Bộ não correlate thông tin, suy luận, hiểu context, đề xuất hành động. Chậm hơn, nhưng xử lý được sự phức tạp.

**Cái sai lầm phổ biến nhất là nhầm lẫn hai tầng này với nhau.**

Có team dùng AI cho việc của tầng 1 - kết quả là hệ thống trở nên unpredictable, đôi khi miss alert quan trọng.

Có team cố gắng dùng rule-based cho việc của tầng 2 - kết quả là hàng trăm rule phức tạp mà vẫn không cover hết case.

Hãy đi vào chi tiết từng tầng.

---

## Tầng 1: Nơi cron job là vua

Nhớ bài về **dao mổ và dao gọt táo** không? Đây là lãnh thổ của dao gọt táo.

Những task ở tầng này bạn muốn chạy ổn định 99.99%. Không bao giờ miss một alert. Không surprise. Đơn giản, deterministic, và nhàm chán - đúng theo cách nó nên như vậy.

**Health check mỗi 30 giây.** Job ping từng service, response 200 thì OK, khác thì đếm consecutive failure. Đạt 3 lần liên tiếp thì alert. Logic là `if status != 200: alert()`. Không cần AI hiểu gì cả.

**Theo dõi resource mỗi phút.** CPU > 85% liên tục 5 phút thì warning, > 95% thì critical. Memory, disk tương tự. Threshold comparison đơn giản. Viết một lần, chạy mãi mãi.

**Certificate expiry mỗi ngày.** Còn 30 ngày thì email warning, còn 7 ngày thì critical. Phép tính ngày tháng. Không có gì mơ hồ.

**Queue depth mỗi phút.** RabbitMQ > 10,000 message và consumer < 3, alert "queue đang backlog." Logic cố định. Không cần hiểu message trong queue là gì.

**Database replication lag mỗi 30 giây.** Lag > 5 giây thì warning, > 30 giây thì critical. Số so với số. Deterministic hoàn toàn.

**Synthetic transaction mỗi 5 phút.** Chạy kịch bản cố định: tạo test order → verify xuất hiện trong DB → cleanup. Pass hoặc fail. Không có vùng xám.

Hùng từng nói với team: "Những job này giống như hệ thần kinh của cơ thể. Bạn không cần suy nghĩ để rụt tay lại khi chạm lửa. Phản xạ phải nhanh và đáng tin cậy. Nếu bạn phải 'suy nghĩ' mỗi lần chạm lửa, bạn đã bị bỏng rồi."

**Pattern chung:** đầu vào là số, logic là so sánh, output là pass/fail. Không cần hiểu ngữ nghĩa, không cần context. Đây là việc của cron job.

---

## Tầng 2: Nơi AI Agent tỏa sáng

Quay lại đêm hôm đó với 47 alert.

Cron job đã làm đúng việc của nó - detect được tất cả 47 vấn đề. Nhưng nó không thể làm điều mà Hiếu đã làm: **nhìn vào timeline, correlate thông tin, và nhận ra đâu là root cause, đâu là domino effect.**

Đây là lãnh thổ của AI Agent.

**Correlate alert và tìm root cause.** Agent đọc toàn bộ 47 alert, phân tích timeline: DB connection pool exhausted lúc 3:01, sau đó API timeout lúc 3:02, queue backlog lúc 3:03, 5xx error lúc 3:04. Agent tổng hợp thành một message: *"Root cause khả năng cao là DB connection pool. 46 alert còn lại là domino effect. Recommend: check recent deployment và connection pool config."*

Mỗi incident, tổ hợp alert khác nhau. Không rule nào cover hết.

**Đọc hiểu error log.** Hệ thống sinh ra hàng nghìn dòng error log mỗi ngày. Cron job đếm được số lượng, nhưng không biết dòng nào quan trọng. Agent phân biệt: `NullPointerException at PaymentService` xuất hiện 200 lần trong 10 phút - bug nghiêm trọng, ảnh hưởng user. `TimeoutException at RecommendationService` 500 lần - severity thấp, chỉ ảnh hưởng feature gợi ý không critical.

Agent phân loại dựa trên **business impact**, không chỉ đếm số.

**Gợi ý runbook action.** 3 giờ sáng, Tiến đầu óc chưa tỉnh. Agent nói: *"Triệu chứng này giống incident INC-4521 tháng trước. Lần đó root cause là connection pool config sau deployment. Step 1: check recent deployment. Step 2: verify connection pool setting. Step 3: nếu đúng thì rollback."*

Agent không chỉ match keyword mà **hiểu sự tương đồng về pattern**.

**Viết incident summary tự động.** Sau incident, team phải viết postmortem. Thông tin nằm rải rác - alert history từ PagerDuty, deployment log từ CI/CD, Slack conversation, git commit. Agent thu thập từ nhiều nguồn, tổng hợp thành draft với timeline chính xác đến phút.

Mỗi incident diễn biến hoàn toàn khác nhau - không template hóa được.

**Pattern chung:** đầu vào là unstructured data (log, alert stream, conversation), cần hiểu context để quyết định, output là recommendation có nuance. Đây là việc của AI Agent.

---

## Câu chuyện về lần AI suýt giết production

Sau buổi postmortem, Hùng quyết định thử nghiệm AI trong monitoring. Bạn ấy setup một Agent để "thông minh hóa" alerting - Agent sẽ đánh giá mỗi alert và quyết định có nên escalate hay không.

Ban đầu mọi thứ hoạt động tốt. Agent suppress được nhiều false positive. Team ít bị làm phiền bởi alert noise hơn. Hùng tự hào demo trong sprint review.

Cho đến một đêm thứ Bảy.

Trường - junior engineer đang on-call - không nhận được alert nào cả đêm. Sáng Chủ nhật, khách hàng gọi điện: "App không hoạt động từ 2 giờ sáng."

Khi check lại, team phát hiện: hệ thống monitoring truyền thống **đã bắt được lỗi ngay từ 2:03 sáng**. Nhưng AI Agent đánh giá đó là false positive - vì pattern "tương tự" với một số alert không quan trọng trước đó - và suppress nó đi.

Production sập 6 tiếng. Không ai biết.

Hùng ngồi viết postmortem với vẻ mặt đau khổ. Tiến ngồi cạnh, không nói gì.

Cuối cùng Hùng viết: *"Root cause: AI Agent suppress nhầm critical alert. Lesson learned: AI trong monitoring nên là cố vấn, không bao giờ nên là gatekeeper."*

---

## Nguyên tắc vàng mà team học được

Sau incident đó, team đặt ra mấy nguyên tắc:

**1. AI không bao giờ được quyền suppress alert.**

Agent có thể nói "alert này có thể là false positive," nhưng alert vẫn phải đến tay on-call engineer. Quyền quyết định cuối cùng nằm ở người, không nằm ở model.

**2. Tầng 1 phải độc lập với AI.**

Nếu AI Agent chết, tầng 1 vẫn phải hoạt động bình thường. Cron job vẫn ping health check, vẫn so sánh threshold, vẫn gửi alert. AI là lớp bổ sung, không phải lớp thay thế.

**3. AI càng gần production decision thì quyền hạn càng phải nhỏ.**

Ở tầng post-incident (viết postmortem, phân tích pattern), AI có thể tự do. Ở tầng response (gợi ý runbook), AI chỉ gợi ý. Ở tầng alert gốc, AI không nên can thiệp.

**4. Đừng dùng AI để bù đắp cho alert config kém.**

Hiếu từng nói: "Nhiều team nghĩ cần AI vì hệ thống có quá nhiều noise. Nhưng câu hỏi thật sự là: tại sao bạn có nhiều noise vậy? Threshold có hợp lý không? Alert có đúng granularity không? 70-80% vấn đề alert noise có thể giải quyết bằng tuning rule tốt hơn - không cần AI."

Dùng AI để lọc alert noise giống như **dùng thuốc giảm đau thay vì chữa bệnh**.

---

## Kiến trúc monitoring mà team chọn

Sau tất cả những bài học đó, đây là kiến trúc mà team của Hùng xây dựng:

**Tầng 1: Thu thập và Alert (100% rule-based)**

Prometheus, Grafana, AlertManager, PagerDuty. **Không có AI ở đây.** Tầng này phải đơn giản và đáng tin cậy nhất có thể. Đây là nền tảng mà team đặt cược production lên.

**Tầng 2: Enrichment và Correlation (AI Agent)**

Agent nhận alert stream từ tầng dưới, correlate, thêm context, đánh giá severity thực sự, group related alerts lại. **Nhưng không có quyền suppress hay modify alert gốc.** Nó chỉ thêm một lớp thông tin bên trên.

**Tầng 3: Response Assistance (AI Agent)**

Gợi ý runbook, tìm incident tương tự trong quá khứ, draft communication cho stakeholder. **Nhưng mọi hành động thực tế** (restart service, rollback deploy) **vẫn do engineer thực hiện.**

**Tầng 4: Post-incident (AI Agent)**

Draft postmortem, trích xuất action items, identify pattern lặp lại. Đây là chỗ **ít rủi ro nhất** cho AI - sai cũng không ảnh hưởng production.

Triết lý xuyên suốt: **AI là cố vấn ngồi cạnh, không phải autopilot.**

---

## Điều tôi lo ngại nhất

Có một điều tôi hay nhắc team: **đừng để mất khả năng debug bằng tay.**

Debugging là kỹ năng cần luyện. Bạn cần nhìn log rồi tự suy luận, tự đặt hypothesis, tự verify. Nếu mỗi lần có incident, engineer chỉ hỏi Agent rồi làm theo gợi ý, thì sau 1-2 năm, khi Agent gợi ý sai hoặc gặp tình huống nó chưa từng thấy, **không ai trong team có đủ kinh nghiệm để tự xử lý nữa**.

Tiến sau đêm 47 alert đó đã học được rất nhiều. Bạn ấy bắt đầu đọc log kỹ hơn, học cách correlate thông tin, xây dựng mental model về hệ thống. Mấy tháng sau, Tiến đã có thể nhìn ra pattern như Hiếu.

Nếu đêm đó có AI Agent làm hết, Tiến có học được gì không?

---

## Lời kết

Quay lại buổi postmortem sau đêm 47 alert.

Hùng kết thúc bằng câu: "Monitoring là nơi mà boring technology wins. Cron job chạy health check mỗi 30 giây không sexy, không có gì để demo, nhưng nó cứu production lúc 3h sáng đáng tin hơn bất kỳ AI Agent nào."

Tiến gật đầu. Hiếu gật đầu.

"Hãy xây nền tảng boring cho thật vững," Hùng nói tiếp, "rồi mới thêm AI lên trên như một lớp gia tăng giá trị. Đừng bao giờ dùng AI để thay thế nền tảng đó."

Đó là bài học mà team học được bằng cách khó khăn nhất - qua những đêm mất ngủ và những incident đau thương.

Và đó cũng là bài học tôi muốn chia sẻ với bạn.

---

*Boring is beautiful. Reliable is everything.*

*Và đừng bao giờ để AI trở thành single point of failure trong monitoring stack.*
